{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. EEGNet - compact CNN designed for EEG (Lawhern et al., 2018)\n",
    "    - learns frequency filters \n",
    "    - learns spatial filters like CSP\n",
    "    - efficient feature combo\n",
    "    - only 2k parameters, reduces overfitting on small datasets\n",
    "2. Training with early stopping\n",
    "3. Comparison with classical CSP+SVM\n",
    "4. Learned feature visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, cohen_kappa_score, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.notebook import tqdm\n",
    "import json\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Add src to path\n",
    "sys.path.insert(0, str(Path.cwd().parent / 'src'))\n",
    "\n",
    "from models import EEGNet, EEGNetTrainer, create_eeg_dataloaders\n",
    "from visualization import set_style, plot_training_history, plot_confusion_matrix, CLASS_NAMES\n",
    "\n",
    "set_style()\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "\n",
    "# Device\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "PROCESSED_DIR = Path('../data/processed')\n",
    "RESULTS_DIR = Path('../results')\n",
    "FIGURES_DIR = Path('../figures')\n",
    "RESULTS_DIR.mkdir(exist_ok=True)\n",
    "FIGURES_DIR.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load preprocessed data\n",
    "data = np.load(PROCESSED_DIR / 'preprocessed_data.npz', allow_pickle=True)\n",
    "\n",
    "X_train_all = data['X_train']\n",
    "y_train_all = data['y_train']\n",
    "subjects_train = data['subjects_train']\n",
    "\n",
    "X_test_all = data['X_test']\n",
    "y_test_all = data['y_test']\n",
    "subjects_test = data['subjects_test']\n",
    "\n",
    "n_channels = X_train_all.shape[1]\n",
    "n_times = X_train_all.shape[2]\n",
    "n_classes = len(np.unique(y_train_all))\n",
    "\n",
    "print(f\"Training data: {X_train_all.shape}\")\n",
    "print(f\"Test data: {X_test_all.shape}\")\n",
    "print(f\"Channels: {n_channels}, Time points: {n_times}, Classes: {n_classes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Architecture\n",
    "\n",
    "Input: (batch, 1, channels, times)\n",
    "  │\n",
    "  ├─► Conv2D(1→F1, kernel=(1, 64))      # Temporal filtering\n",
    "  ├─► BatchNorm\n",
    "  │\n",
    "  ├─► DepthwiseConv2D(F1→F1*D, kernel=(channels, 1))  # Spatial filtering (like CSP)\n",
    "  ├─► BatchNorm → ELU → AvgPool(1,4) → Dropout\n",
    "  │\n",
    "  ├─► SeparableConv2D(F1*D→F2, kernel=(1, 16))  # Feature combination\n",
    "  ├─► BatchNorm → ELU → AvgPool(1,8) → Dropout\n",
    "  │\n",
    "  └─► Flatten → Dense(n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect EEGNet architecture\n",
    "model_example = EEGNet(\n",
    "    n_channels=n_channels,\n",
    "    n_times=n_times,\n",
    "    n_classes=n_classes,\n",
    "    F1=8,      # Temporal filters\n",
    "    D=2,       # Depth multiplier\n",
    "    F2=16,     # Pointwise filters\n",
    "    dropout=0.5\n",
    ")\n",
    "\n",
    "print(model_example)\n",
    "print(f\"\\nTotal parameters: {sum(p.numel() for p in model_example.parameters()):,}\")\n",
    "print(f\"Trainable parameters: {sum(p.numel() for p in model_example.parameters() if p.requires_grad):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test forward pass\n",
    "x_test = torch.randn(4, n_channels, n_times)  # batch of 4\n",
    "with torch.no_grad():\n",
    "    out = model_example(x_test)\n",
    "print(f\"Input shape: {x_test.shape}\")\n",
    "print(f\"Output shape: {out.shape}\")\n",
    "print(f\"Output (logits): {out[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Within-subject training (same as classical approach)\n",
    "# Hyperparameters\n",
    "HPARAMS = {\n",
    "    'F1': 8,\n",
    "    'D': 2,\n",
    "    'F2': 16,\n",
    "    'dropout': 0.5,\n",
    "    'learning_rate': 1e-3,\n",
    "    'batch_size': 32,\n",
    "    'epochs': 200,\n",
    "    'early_stopping_patience': 20,\n",
    "    'val_split': 0.2,  # Use 20% of training for validation\n",
    "}\n",
    "\n",
    "print(\"Hyperparameters:\")\n",
    "for k, v in HPARAMS.items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_subject_eegnet(subj_id, X_train_all, y_train_all, subjects_train,\n",
    "                         X_test_all, y_test_all, subjects_test, hparams, device='cpu'):\n",
    "    \"\"\"\n",
    "    Train EEGNet for a single subject.\n",
    "    Returns dict with results and trained model.\n",
    "    \"\"\"\n",
    "    # Get subject data\n",
    "    mask_train = subjects_train == subj_id\n",
    "    mask_test = subjects_test == subj_id\n",
    "    \n",
    "    X_train = X_train_all[mask_train].astype(np.float32)\n",
    "    y_train = y_train_all[mask_train].astype(np.int64)\n",
    "    X_test = X_test_all[mask_test].astype(np.float32)\n",
    "    y_test = y_test_all[mask_test].astype(np.int64)\n",
    "    \n",
    "    # Normalize per channel (z-score)\n",
    "    mean = X_train.mean(axis=(0, 2), keepdims=True)\n",
    "    std = X_train.std(axis=(0, 2), keepdims=True) + 1e-8\n",
    "    X_train = (X_train - mean) / std\n",
    "    X_test = (X_test - mean) / std\n",
    "    \n",
    "    # Split training into train/val\n",
    "    X_tr, X_val, y_tr, y_val = train_test_split(\n",
    "        X_train, y_train, \n",
    "        test_size=hparams['val_split'],\n",
    "        stratify=y_train,\n",
    "        random_state=SEED\n",
    "    )\n",
    "    \n",
    "    # Create dataloaders\n",
    "    train_loader, val_loader = create_eeg_dataloaders(\n",
    "        X_tr, y_tr, X_val, y_val,\n",
    "        batch_size=hparams['batch_size']\n",
    "    )\n",
    "    \n",
    "    # Create model\n",
    "    model = EEGNet(\n",
    "        n_channels=X_train.shape[1],\n",
    "        n_times=X_train.shape[2],\n",
    "        n_classes=len(np.unique(y_train)),\n",
    "        F1=hparams['F1'],\n",
    "        D=hparams['D'],\n",
    "        F2=hparams['F2'],\n",
    "        dropout=hparams['dropout']\n",
    "    )\n",
    "    \n",
    "    # Train\n",
    "    trainer = EEGNetTrainer(\n",
    "        model=model,\n",
    "        device=device,\n",
    "        learning_rate=hparams['learning_rate']\n",
    "    )\n",
    "    \n",
    "    history = trainer.fit(\n",
    "        train_loader, val_loader,\n",
    "        epochs=hparams['epochs'],\n",
    "        early_stopping_patience=hparams['early_stopping_patience'],\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    y_pred = trainer.predict(X_test)\n",
    "    \n",
    "    return {\n",
    "        'subject': subj_id,\n",
    "        'n_train': len(y_train),\n",
    "        'n_test': len(y_test),\n",
    "        'accuracy': accuracy_score(y_test, y_pred),\n",
    "        'kappa': cohen_kappa_score(y_test, y_pred),\n",
    "        'y_true': y_test,\n",
    "        'y_pred': y_pred,\n",
    "        'history': history,\n",
    "        'model': model,\n",
    "        'best_val_acc': max(history['val_acc']),\n",
    "        'epochs_trained': len(history['train_loss'])\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train on all subjects\n",
    "all_results = []\n",
    "\n",
    "print(\"Training EEGNet for each subject...\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'Subject':<10} {'Train':<8} {'Test':<8} {'Epochs':<10} {'Val Acc':<12} {'Test Acc':<12} {'Kappa':<10}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for subj in range(1, 10):\n",
    "    results = train_subject_eegnet(\n",
    "        subj, X_train_all, y_train_all, subjects_train,\n",
    "        X_test_all, y_test_all, subjects_test,\n",
    "        HPARAMS, device=DEVICE\n",
    "    )\n",
    "    all_results.append(results)\n",
    "    \n",
    "    print(f\"{subj:<10} {results['n_train']:<8} {results['n_test']:<8} \"\n",
    "          f\"{results['epochs_trained']:<10} {results['best_val_acc']:<12.2%} \"\n",
    "          f\"{results['accuracy']:<12.2%} {results['kappa']:<10.3f}\")\n",
    "\n",
    "print(\"-\" * 70)\n",
    "\n",
    "# Averages\n",
    "avg_acc = np.mean([r['accuracy'] for r in all_results])\n",
    "avg_kappa = np.mean([r['kappa'] for r in all_results])\n",
    "std_acc = np.std([r['accuracy'] for r in all_results])\n",
    "\n",
    "print(f\"{'Mean':<10} {'':<8} {'':<8} {'':<10} {'':<12} {avg_acc:<12.2%} {avg_kappa:<10.3f}\")\n",
    "print(f\"{'Std':<10} {'':<8} {'':<8} {'':<10} {'':<12} {std_acc:<12.2%}\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves for all subjects\n",
    "fig, axes = plt.subplots(3, 3, figsize=(14, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, r in enumerate(all_results):\n",
    "    ax = axes[idx]\n",
    "    epochs = range(1, len(r['history']['train_acc']) + 1)\n",
    "    \n",
    "    ax.plot(epochs, r['history']['train_acc'], 'b-', label='Train', alpha=0.7)\n",
    "    ax.plot(epochs, r['history']['val_acc'], 'r-', label='Val', alpha=0.7)\n",
    "    ax.axhline(r['accuracy'], color='g', linestyle='--', label=f'Test: {r[\"accuracy\"]:.1%}')\n",
    "    ax.axhline(0.25, color='gray', linestyle=':', alpha=0.5)\n",
    "    \n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('Accuracy')\n",
    "    ax.set_title(f'Subject {r[\"subject\"]}')\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.legend(loc='lower right', fontsize=8)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / 'eegnet_training_curves.png', dpi=150, bbox_inches='tight');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate confusion matrix\n",
    "y_true_all_test = np.concatenate([r['y_true'] for r in all_results])\n",
    "y_pred_all_test = np.concatenate([r['y_pred'] for r in all_results])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "cm = confusion_matrix(y_true_all_test, y_pred_all_test, normalize='true')\n",
    "sns.heatmap(cm, annot=True, fmt='.2%', cmap='Blues',\n",
    "            xticklabels=CLASS_NAMES, yticklabels=CLASS_NAMES,\n",
    "            ax=ax, square=True, cbar_kws={'label': 'Proportion'})\n",
    "ax.set_xlabel('Predicted')\n",
    "ax.set_ylabel('True')\n",
    "ax.set_title(f'EEGNet Confusion Matrix (Acc: {avg_acc:.1%})')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / 'eegnet_confusion_matrix.png', dpi=150, bbox_inches='tight');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-class accuracy\n",
    "print(\"Per-class performance (EEGNet):\")\n",
    "print(\"=\" * 50)\n",
    "print(classification_report(y_true_all_test, y_pred_all_test,\n",
    "                            target_names=CLASS_NAMES, digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load classical results\n",
    "classical_results_path = RESULTS_DIR / 'classical_ml_results.json'\n",
    "\n",
    "if classical_results_path.exists():\n",
    "    with open(classical_results_path) as f:\n",
    "        classical_results = json.load(f)\n",
    "    \n",
    "    # Compare per-subject\n",
    "    comparison_data = []\n",
    "    for i, r in enumerate(all_results):\n",
    "        comparison_data.append({\n",
    "            'Subject': f'S{r[\"subject\"]}',\n",
    "            'CSP+LDA': classical_results['lda']['per_subject_accuracy'][i],\n",
    "            'CSP+SVM': classical_results['svm']['per_subject_accuracy'][i],\n",
    "            'EEGNet': r['accuracy']\n",
    "        })\n",
    "    \n",
    "    import pandas as pd\n",
    "    df_comparison = pd.DataFrame(comparison_data)\n",
    "    print(\"Per-subject comparison:\")\n",
    "    print(df_comparison.to_string(index=False))\n",
    "    \n",
    "    # Summary\n",
    "    print(f\"\\nMean accuracy:\")\n",
    "    print(f\"  CSP+LDA: {classical_results['lda']['mean_accuracy']:.2%}\")\n",
    "    print(f\"  CSP+SVM: {classical_results['svm']['mean_accuracy']:.2%}\")\n",
    "    print(f\"  EEGNet:  {avg_acc:.2%}\")\n",
    "else:\n",
    "    print(\"Run notebook 03 first to get classical results for comparison\")\n",
    "    df_comparison = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of comparison\n",
    "if df_comparison is not None:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Bar chart per subject\n",
    "    x = np.arange(len(df_comparison))\n",
    "    width = 0.25\n",
    "    \n",
    "    axes[0].bar(x - width, df_comparison['CSP+LDA'], width, label='CSP+LDA', color='steelblue')\n",
    "    axes[0].bar(x, df_comparison['CSP+SVM'], width, label='CSP+SVM', color='coral')\n",
    "    axes[0].bar(x + width, df_comparison['EEGNet'], width, label='EEGNet', color='forestgreen')\n",
    "    \n",
    "    axes[0].axhline(0.25, color='gray', linestyle=':', label='Chance')\n",
    "    axes[0].set_xlabel('Subject')\n",
    "    axes[0].set_ylabel('Accuracy')\n",
    "    axes[0].set_title('Method Comparison by Subject')\n",
    "    axes[0].set_xticks(x)\n",
    "    axes[0].set_xticklabels(df_comparison['Subject'])\n",
    "    axes[0].set_ylim(0, 1)\n",
    "    axes[0].legend()\n",
    "    \n",
    "    # Summary bar chart\n",
    "    methods = ['CSP+LDA', 'CSP+SVM', 'EEGNet']\n",
    "    means = [\n",
    "        classical_results['lda']['mean_accuracy'],\n",
    "        classical_results['svm']['mean_accuracy'],\n",
    "        avg_acc\n",
    "    ]\n",
    "    stds = [\n",
    "        classical_results['lda']['std_accuracy'],\n",
    "        classical_results['svm']['std_accuracy'],\n",
    "        std_acc\n",
    "    ]\n",
    "    colors = ['steelblue', 'coral', 'forestgreen']\n",
    "    \n",
    "    bars = axes[1].bar(methods, means, yerr=stds, capsize=5, color=colors, edgecolor='black')\n",
    "    axes[1].axhline(0.25, color='gray', linestyle=':', label='Chance')\n",
    "    axes[1].set_ylabel('Accuracy')\n",
    "    axes[1].set_title('Mean Accuracy Comparison')\n",
    "    axes[1].set_ylim(0, 1)\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, mean, std in zip(bars, means, stds):\n",
    "        axes[1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + std + 0.02,\n",
    "                     f'{mean:.1%}', ha='center', va='bottom', fontsize=11)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(FIGURES_DIR / 'method_comparison.png', dpi=150, bbox_inches='tight');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize learned features and examine what EEGNet learned in its filters\n",
    "# Get the best performing subject's model\n",
    "best_subj_idx = np.argmax([r['accuracy'] for r in all_results])\n",
    "best_model = all_results[best_subj_idx]['model']\n",
    "best_subj = all_results[best_subj_idx]['subject']\n",
    "\n",
    "print(f\"Analyzing model from Subject {best_subj} (accuracy: {all_results[best_subj_idx]['accuracy']:.1%})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize temporal filters (first conv layer)\n",
    "conv1_weights = best_model.conv1.weight.detach().cpu().numpy()\n",
    "print(f\"Temporal filter shape: {conv1_weights.shape}\")\n",
    "print(f\"  (F1 filters, 1 input channel, 1 height, kernel_length)\")\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, figsize=(14, 5))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i in range(min(8, conv1_weights.shape[0])):\n",
    "    kernel = conv1_weights[i, 0, 0, :]\n",
    "    axes[i].plot(kernel, 'b-', linewidth=1.5)\n",
    "    axes[i].set_title(f'Temporal Filter {i+1}')\n",
    "    axes[i].set_xlabel('Sample')\n",
    "    axes[i].axhline(0, color='gray', linestyle='--', alpha=0.5)\n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "\n",
    "fig.suptitle('Learned Temporal Filters (Conv1)', y=1.02)\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frequency response of temporal filters\n",
    "from scipy import signal\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, figsize=(14, 5))\n",
    "axes = axes.flatten()\n",
    "\n",
    "sfreq = 250  # Hz\n",
    "\n",
    "for i in range(min(8, conv1_weights.shape[0])):\n",
    "    kernel = conv1_weights[i, 0, 0, :]\n",
    "    \n",
    "    # Compute frequency response\n",
    "    freqs, response = signal.freqz(kernel, worN=512, fs=sfreq)\n",
    "    \n",
    "    axes[i].plot(freqs, np.abs(response), 'b-', linewidth=1.5)\n",
    "    axes[i].set_title(f'Filter {i+1}')\n",
    "    axes[i].set_xlabel('Frequency (Hz)')\n",
    "    axes[i].set_ylabel('Magnitude')\n",
    "    axes[i].set_xlim(0, 50)\n",
    "    axes[i].axvspan(8, 12, alpha=0.2, color='blue', label='mu')\n",
    "    axes[i].axvspan(13, 30, alpha=0.2, color='red', label='beta')\n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "\n",
    "fig.suptitle('Frequency Response of Learned Temporal Filters', y=1.02)\n",
    "plt.tight_layout();\n",
    "\n",
    "print(\"\\nNote: Filters should show sensitivity to mu (8-12 Hz) and beta (13-30 Hz) bands\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize spatial filters (depthwise conv - like CSP)\n",
    "import mne\n",
    "from preprocessing import CHANNEL_NAMES\n",
    "\n",
    "conv2_weights = best_model.conv2.weight.detach().cpu().numpy()\n",
    "print(f\"Spatial filter shape: {conv2_weights.shape}\")\n",
    "print(f\"  (F1*D output filters, 1 per group, n_channels, 1)\")\n",
    "\n",
    "# Create MNE info for topomaps\n",
    "info = mne.create_info(ch_names=CHANNEL_NAMES, sfreq=250, ch_types='eeg')\n",
    "montage = mne.channels.make_standard_montage('standard_1020')\n",
    "info.set_montage(montage)\n",
    "\n",
    "# Plot first 8 spatial filters\n",
    "fig, axes = plt.subplots(2, 4, figsize=(14, 6))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i in range(min(8, conv2_weights.shape[0])):\n",
    "    # Get spatial weights for this filter\n",
    "    # Shape is (out_channels, in_channels/groups, height, width)\n",
    "    # For depthwise, in_channels/groups = 1\n",
    "    spatial_weights = conv2_weights[i, 0, :, 0]\n",
    "    \n",
    "    mne.viz.plot_topomap(spatial_weights, info, axes=axes[i], show=False)\n",
    "    axes[i].set_title(f'Spatial Filter {i+1}')\n",
    "\n",
    "fig.suptitle('Learned Spatial Filters (Like CSP Patterns)', y=1.02)\n",
    "plt.tight_layout();\n",
    "plt.savefig(FIGURES_DIR / 'eegnet_spatial_filters.png', dpi=150, bbox_inches='tight');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results summary\n",
    "eegnet_results = {\n",
    "    'method': 'EEGNet',\n",
    "    'hyperparameters': HPARAMS,\n",
    "    'subjects': [r['subject'] for r in all_results],\n",
    "    'per_subject_accuracy': [r['accuracy'] for r in all_results],\n",
    "    'per_subject_kappa': [r['kappa'] for r in all_results],\n",
    "    'mean_accuracy': float(avg_acc),\n",
    "    'std_accuracy': float(std_acc),\n",
    "    'mean_kappa': float(avg_kappa),\n",
    "    'epochs_trained': [r['epochs_trained'] for r in all_results]\n",
    "}\n",
    "\n",
    "with open(RESULTS_DIR / 'eegnet_results.json', 'w') as f:\n",
    "    json.dump(eegnet_results, f, indent=2)\n",
    "\n",
    "print(f\"Results saved to {RESULTS_DIR / 'eegnet_results.json'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save predictions\n",
    "np.savez(\n",
    "    RESULTS_DIR / 'eegnet_predictions.npz',\n",
    "    y_true=y_true_all_test,\n",
    "    y_pred=y_pred_all_test,\n",
    "    subjects=np.concatenate([np.full(len(r['y_true']), r['subject']) for r in all_results])\n",
    ")\n",
    "\n",
    "print(f\"Predictions saved to {RESULTS_DIR / 'eegnet_predictions.npz'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save best model\n",
    "torch.save({\n",
    "    'model_state_dict': best_model.state_dict(),\n",
    "    'subject': best_subj,\n",
    "    'accuracy': all_results[best_subj_idx]['accuracy'],\n",
    "    'hyperparameters': HPARAMS,\n",
    "    'n_channels': n_channels,\n",
    "    'n_times': n_times,\n",
    "    'n_classes': n_classes\n",
    "}, RESULTS_DIR / 'best_eegnet_model.pt')\n",
    "\n",
    "print(f\"Best model saved to {RESULTS_DIR / 'best_eegnet_model.pt'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Method | Mean Accuracy | Std | Kappa |\n",
    "|--------|--------------|-----|-------|\n",
    "| CSP + LDA | ~65-70% | - | ~0.55 |\n",
    "| CSP + SVM | ~70-75% | - | ~0.60 |\n",
    "| EEGNet | ~70-78% | - | ~0.65 |\n",
    "| Chance | 25% | - | 0.00 |\n",
    "\n",
    "Potential Improvements\n",
    "\n",
    "- Data augmentation (time shift, noise)\n",
    "- Cross-subject pretraining + fine-tuning\n",
    "- Attention mechanisms (transformers)\n",
    "- Larger models with more data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
